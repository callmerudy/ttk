<html>

<head>

<style>
body { width: 820px; }
img { margin-top: 10pt; margin-left: 10pt; border: thin solid grey; display: block; }
.navigation { font-size: 120%; }
.spacy li { margin-top: 10pt; margin-bottom: 10pt; }
.tight li { margin-top: 4pt; margin-bottom: 4pt; }
.example { padding: 5pt; background: #eeeeee; border: thin dotted grey; }
.image { margin: 5pt; margin-left: 15pt; padding: 0pt; }
.indent { margin-left: 10pt; }
.embedded { margin-right: 5pt; margin-bottom: 5pt; }
h1 { border-bottom: thick solid green; }
h2 { margin-top: 25pt; border-bottom: medium solid green; }
h3 { margin-top: 15pt; border-bottom: thin solid green; }
code { font-size: 125%; }
table { border: thin solid grey; border-collapse: collapse; }
table td { border: thin solid grey; vertical-align: top; padding: 3pt; }
table tr.tableheader { background-color: #eeeeee; }
table.spacy { margin-top: 10pt; margin-bottom: 10pt; }
table.pythonclass { width: 500pt; }
table.pythonclass td { border-style: none; padding-left: 6pt; }
table.pythonclass td.name { font-weight: bold; border: thin solid grey; padding-left: 3pt;}
table.pythonclass td.name { background-color: #eeeeee; }
table.pythonclass td.attribute { }
</style>

</head>

<body>

<h1>Overview of the TARSQI Toolkit</h1>

<p>Marc Verhagen, December 2015<p>

<p>This document gives a high-level overview of the TARSQI Toolkit code.</p>

<p class="navigation">
[ <a href="#toplevel">toplevel</a>
| <a href="#tags">tags</a>
| <a href="#preprocessor">preprocessor</a>
| <a href="#gutime">gutime</a>
| <a href="#evita">evita</a>
| <a href="#slinket">slinket</a>
| <a href="#s2t">s2t</a>
| <a href="#blinker">blinker</a>
| <a href="#classifier">classifier</a>
| <a href="#sputlink">sputlink</a>
]
</p>


<!----------------------------------------------------------------------------->
<a name="toplevel"/>
<h2>1. Top-level Processing</h2>
<!----------------------------------------------------------------------------->

<p>All code, barring a few scripts, lives in the code directory
inside the Tarsqi distribution. All paths given are relative to that path.</p>

<p>The top-level processing chain is implemented in the Tarsqi class in
tarsqi.py, which upon initialization does the following:</p>

<ol class="spacy">

<li>Read the parameters. These are read from settings.txt but can
also be overridden on the command line invocation.

<li>Create a pipeline from the --pipeline parameter. A pipeline is
a list of pairs where each pair has the name of the component and the Python
class that implements it.

<li>Select the source parser. There are four parsers defined in
docmodel.parsers: DefaultParser, TimebankParser, ATEEParser and RTE3Parser. The
choice of parser is guided by the --genre parameter.

</ol>

<p>There is an instance of the Tarsqi class for each document processed. Actual
processing occurs through the Tarsqi.process() method, which does the following
things:</p>

<ol class="spacy">

<li>The class docmodel.source_parser.SourceParser parses the input file using
parse_file(filename) and returns an instance of docmodel.source_parser.SourceDoc
which includes two instance variables: text and tags.

<img src="images/sourceparser.png"/ height="75">

<table class="pythonclass indent spacy">
<tr>
  <td class="name" colspan="2">docmodel.source_parser.SourceDoc</td>
</tr>
<tr>
  <td class="attribute">text</td>
  <td>unicode string</td>
</tr>  
<tr>
  <td class="attribute">tags</td>
  <td>
    <table class="pythonclass embedded">
      <tr>
	<td class="name" colspan="2">docmodel.source_parser.TagRepository</td>
      </tr>
      <tr>
	<td class="attribute">tags</td>
	<td>list of instances of docmodel.source_parser.Tag</td>
      </tr>  
      <tr>
	<td class="attribute">opening_tags</td>
	<td>dictionary of instances of docmodel.source_parser.Tag, indexed on
	opening offsets</td>
      </tr>  
      <tr>
	<td class="attribute">closing_tags</td>
	<td>dictionary of tag names, indexed on closing offsets and opening
	offsets</td>
      </tr>  
    </table>
  </td>
</tr>  
</table>

The input is assumed to be an XML file with inline XML tags and the SourceParser
turns it into the primary text data (text without tags) and a dictionary of
class TagRepository which has tags with character offsets pointing into the
primary data. Both instance variables are intended to be read-only. That is,
after the SourceDoc is created the primary data string never changes and tags
are not added to the TagRepository. Here is a minimal example as an
illustration:

<pre class="example indent">
&lt;?xml version="1.0" ?>
&lt;text>One &lt;noun>tag&lt;/noun> only.&lt;/text>
</pre>

<p>For this text, the tags list is as follows:</p>

<pre class="example indent">
[ &lt;docmodel.source_parser.Tag instance at 0x108d6c518>, 
  &lt;docmodel.source_parser.Tag instance at 0x108d6c4d0> ]
</pre>

<p>For clarity, here is the same list but with the unhelpful print string for
Tags replaced by a more helpful one:</p>

<pre class="example indent">
[ &lt;Tag text id=1 1:14 {}>,
  &lt;Tag noun id=2 5:8 {}> ]
</pre>

<p>The dictionaries on the two tags are empty, but if the XML tags had
attributes they would end up in there. The two other instance variables on
TagRepository are in effect indexes over the list, giving quick access to the
tags at specified begin or end offsets. The opening tags dictionary is as
follows:</p>

<pre class="example indent">
{ 1: [ &lt;Tag text id=1 1:14 {}> ],
  5: [ &lt;Tag noun id=2 5:8 {}> ] }
</pre>

Which indicates that there are opening tags at positions 1 and 5, and in both
cases there is only one tag at that offset. Instances of Tag contain the name of
the tag, its attributes and its begin and end offsets. The closing tags
dictionary for the above example is:

<pre class="example indent">
{  8: { 5: {u'noun': True} }, 
  14: { 1: {u'text': True} } }
</pre>

This dictionary says that at character offset 8 we close a noun tag that was
opened at offset 5. The TagRepository class has convenience methods to access
tags.

<li>One of the parsers from docmodel.parsers is then used to take the SourceDoc
instance and create an instance of docmodel.document.TarsqiDocument.

<img src="images/defaultparser.png"/ height="75">

<table class="pythonclass indent spacy">
<tr>
  <td class="name" colspan="2">docmodel.document.TarsqiDocument</td>
</tr>
<tr>
  <td class="attribute">source</td>
  <td>an instance of docmodel.source_parser.SourceDoc</td>
</tr>
<tr>
  <td class="attribute">metadata</td>
  <td>a dictionary</td>
</tr>
<tr>
  <td class="attribute">parameters</td>
  <td>a dictionary</td>
</tr>
<tr>
  <td class="attribute">elements</td>
  <td>a list of instances of docmodel.document.TarsqiDocElement</td>
</tr>
</table>

The DefaultParser does some minimal processing of the source document,
extracting a document creation time and putting it in the metadata dictionary
under the 'dct' key and splitting the document into paragraphs and putting each
paragraph in the elements list. In the current implimentation, all elements are
instances of TarsqiDocParagraph, a subclass of TarsqiDocElement, which has the
following structure:

<table class="pythonclass indent spacy">
<tr>
  <td class="name" colspan="2">docmodel.document.TarsqiDocElement</td>
</tr>
<tr>
  <td class="attribute">doc</td>
  <td>an instance of docmodel.document.TarsqiDocument</td>
</tr>  
<tr>
  <td class="attribute">begin</td>
  <td>the beginning offset in the SourceDoc in doc.source</td>
</tr>
<tr>
  <td class="attribute">end</td>
  <td>the ending offset in the SourceDoc in doc.source</td>
</tr>
<tr>
  <td class="attribute">source_tags</td>
  <td>an instance of docmodel.source_parser.TagRepository, containing a subset
  of the tags in the source document, namely those that span part or all of the
  paragraph</td>
</tr>
<tr>
  <td class="attribute">tarsqi_tags</td>
  <td>an instance of docmodel.source_parser.TagRepository, initially empty</td>
</tr>
</table>

In the future, other sub classes of TarsqiDocElement will be added (for example
SectionHeader) and most likely the elements variable will become a tree rather
than a flat list.

<li>Add the parameters from the settings.txt file and the command line options
to the TarsqiDocument. This fills in the parameters dictionary.

<li>Apply components as specified in the pipeline. Recall that on initialization
the Tarsqi class creates a pipeline of components from the user's --pipeline
option. If we had used a command line invocation like

<pre class="example indent">
$ python tarsqi.py --pipeline=PREPROCESSOR,GUTIME,EVITA in.xml out.xml
</pre>

then the pipeline as stored in the Tarsqi instance would be

<pre class="example indent">
[('PREPROCESSOR', &lt;class components.preprocessing.wrapper.PreprocessorWrapper at 0x10514a668>), 
 ('GUTIME', &lt;class components.gutime.wrapper.GUTimeWrapper at 0x1051c36d0>), 
 ('EVITA', &lt;class components.evita.wrapper.EvitaWrapper at 0x1051c3ce8>)]
</pre>

The code for all components is wrapped in special wrapper classes like
PreprocessorWrapper and EvitaWrapper in the example above. All available
wrappers are defined in the COMPONENTS dictiornary in
components.__init__.py. Every wrapper is required to have the following two
methods:

<ol class="tight">
<li>an initialization method that takes the TarsqiDocument as its sole argument</li>
<li>a process() method which has the side effect of changing the TarsqiDocument
instance.</li>
</ol>

Components update the TarsqiDocument by updating the TagRepository instances
inside the TarsqiDocElement instances in the elements list. In some cases,
another data structure is updated first and then the results are exported to the
TarsqiDocElements.

<li>Print results. The Tarsqi class does this by asking the TarsqiDocument to
print the source string and all tags to a document, which in turn is done by
retrieving the source and the source tags from the SourceDoc instance and the
added Tarsqi tags from the TagRepository in the tarsqi_tags attributes on
TarsqiDocElements. The output is written to one file with both the primary data
and the tags. For example, take the example input below.

<pre class="example indent">
&lt;?xml version="1.0" ?>
&lt;text>He sleeps on Friday.&lt;/text>
</pre>

And suppose we have a pipeline that includes the preprocessor, GUTime and
Evita. Then the output will be:

<pre class="example indent">
&lt;ttk>
&lt;text>
He sleeps on Friday.
&lt;/text>
&lt;source_tags>
  &lt;text id="1" begin="1" end="21" />
&lt;/source_tags>
&lt;ttk_tags>
  &lt;TIMEX3 tid="t0" type="DATE" value="20160105" functionInDocument="CREATION_TIME"/>
  &lt;doc_element type="TarsqiDocParagraph" begin="1" end="21">
    &lt;lex id="l1" begin="1" end="3" lemma="he" pos="PP" />
    &lt;ng id="c1" begin="1" end="3" targets="l1 l1" />
    &lt;lex id="l2" begin="4" end="10" lemma="sleep" pos="VBZ" />
    &lt;vg id="c2" begin="4" end="10" targets="l2 l2" />
    &lt;lex id="l3" begin="11" end="13" lemma="on" pos="IN" />
    &lt;lex id="l4" begin="14" end="20" lemma="Friday" pos="NNP" />
    &lt;ng id="c3" begin="14" end="20" targets="l4 l4" />
    &lt;lex id="l5" begin="20" end="21" lemma="." pos="." />
    &lt;s id="s1" begin="1" end="21" targets="l1 l5" />
    &lt;TIMEX3 id="1" begin="14" end="20" type="DATE" value="" />
    &lt;EVENT id="2" begin="4" end="10" polarity="POS" pos="VERB" eiid="ei1"
         tense="PRESENT" eid="e1" aspect="NONE" class="OCCURRENCE" />
  &lt;/doc_element>
&lt;/ttk_tags>
&lt;/ttk>
</pre>

Note that the EVENT tag will actually be printed on one line only, it is split
over two lines here for readability.

</ol>


<!----------------------------------------------------------------------------->
<a name="tags"/>
<h2>2. Tags added by the Tarsqi Toolkit</h2>
<!----------------------------------------------------------------------------->

<p>All Tarsqi tags added by the system have identifiers that are unique to the document
and the tag type. The identifiers consist of a tag-specific prefix and an integer. The
prefixes and the tags they go with are listed in the table below.</p>

<table class="indent">
  <tr class="tableheader">
    <td>tag with identifier example</td>
    <td>component</td>
    <td>description</td>
  </tr>
  <tr>
    <td>&lt;doc_element&gt;</td>
    <td>DefaultParser and subclasses</td>
    <td>paragraphs</td>
  </tr>
  <tr>
    <td>&lt;s id="s1"&gt;</td>
    <td>Tokenizer</td>
    <td>sentences</td>
  </tr>
  <tr>
    <td>&lt;lex id="l12"&gt;</td>
    <td>Tokenizer and Tagger</td>
    <td>tokens with pos and lemma</td>
  </tr>
  <tr>
    <td>&lt;ng id="c1"&gt;</td>
    <td>Chunker</td>
    <td>noun chunks</td>
  </tr>
  <tr>
    <td>&lt;vg id="c2"&gt;</td>
    <td>Chunker</td>
    <td>verb chunks</td>
  </tr>
  <tr>
    <td>&lt;TIMEX3 tid="t3"&gt;</td>
    <td>GUTime</td>
    <td>TimeML time expressions</td>
  </tr>
  <tr>
    <td>&lt;EVENT eid="e23" eiid="ei23"&gt;</td>
    <td>Evita</td>
    <td>events</td>
  </tr>
  <tr>
    <td>&lt;ALINK lid="l31"&gt;</td>
    <td>Slinket</td>
    <td>aspectual links</td>
  </tr>
  <tr>
    <td>&lt;SLINK lid="l32"&gt;</td>
    <td>Slinket</td>
    <td>subordinating links</td>
  </tr>
  <tr>
    <td>&lt;TLINK lid="l33"&gt;</td>
    <td>Blinker, S2T, classifier</td>
    <td>temporal links</td>
  </tr>
</table>

<p>Tags introduced by the preprocessor have lower case names and use the "id"
attribute for the identifiers. TimeML tags are uppercase and introduce their
identifiers with special attributes "tid", "eid", "eiid", and "lid". Noun chunks
and verb chunks share the same prefix and so do the three link types. As per the
TimeML specifications, events have an event identifier and an event instance
identifier, this allows us to deal with those events that have more than one
instance. The latter case is not recognized by the Tarsqi Toolkit however and
the eid and eiid will always have the same integer in it (but not with the same
prefix).</p>


<a name="preprocessor"/>
<h2>3. The Preprocessor</h2>

<p>The PreprocessorWrapper loops through the TarsiDocElements. For each element,
it extracts the source text, runs the tokenizer, tagger and chunker on that text
and then exports the results back to the TarsqiDocElements.</p>

<img src="images/preprocessing.png" height="250"/>

<p>The tokenizer copies the text from the TarsqiDocParagraph (which actually
does not hold the text itself but it has the character offsets in the SourceDoc
and a reference to the SourceDoc) and it returns a list of pairs, where each
pair is either ('&lt;s>', None) for sentence boundaries or a pair of a string
and a TokenizedLex instance, which has instance variables begin, end and
text:</p>

<pre class="example indent">
[('&lt;s>', None),
 (u'Fido', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844998>),
 (u'barks', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844ab8>),
 (u'.', &lt;components.preprocessing.tokenizer.TokenizedLex instance at 0x110844b48>)]
</pre>

<p>Since the tokenizer runs in isolation on the text of an element, assigns
offsets starting at 0. The PreprocessorWrapper adjusts these so that the offsets
point into the correct spot in the full text source of the document.</p>

<p>The tagger is then fed a vertical string consisting of the first element of
all pairs (the s tag or a string):</p>

<pre class="example indent">
&lt;s>
Fido
barks
.
</pre>

The tagger returns a list with as many elements as lines, where each element is
either an s tag or a tab-separated triple of string, part-of-speech tag and
lemma:</p>

<pre class="example indent">
['&lt;s>', 
 'Fido\tNP\tFido', 
 'barks\tVVZ\tbark', 
 '.\tSENT\t.']
</pre>

<p>The PreprocessorWrapper then takes this list and merges it with the list of
pairs that came out of the tokenizer and creates the following structure:</p>

<pre class="example indent">
[[('Fido', 'NNP', 'Fido', 1, 5),
  ('barks', 'VBZ', 'bark', 6, 11),
  ('.', '.', '.', 11, 12)]]
</pre>

<p>Note that the s tags have disappeared and that instead we now have a list of
sublists, with one sublist for each sentence. Another thing that happens at this
transformation stage is some normalization of tag names. The chunker adds ng and
vg tags to the sublists.</p>

<pre class="example indent">
[['&lt;ng>', ('Fido', 'NNP', 'Fido', 1, 5), '&lt;/ng>',
  '&lt;vg>', ('barks', 'VBZ', 'bark', 6, 11), '&lt;/vg>',
  ('.', '.', '.', 11, 12)]]
</pre>

<p>Finally, the information in this data structure is exported to the
tarsqi_tags TagRepository on the TarsqiDocElement. Before the first component in
the pipeline applies, this TagRepository is empty, but with the above input the
preprocessor will append s, ng, vg and lex tags to the tags list and eventually
set it to:</p>

<pre class="example indent">
[ &lt;Tag lex id=l1 1:5 {'lemma': 'Fido', 'pos': 'NNP'}>,
  &lt;Tag ng id=c1 1:5 {}>,
  &lt;Tag lex id=l2 6:11 {'lemma': 'bark', 'pos': 'VBZ'}>,
  &lt;Tag vg id=c2 6:11 {}>,
  &lt;Tag lex id=l3 11:12 {'lemma': '.', 'pos': '.'}>,
  &lt;Tag s id=s1 1:12 {}> ]
</pre>

<p>Notice that the tags are added by a depth-first post-order traversal of the
tree, but this is incidental of the algorithm and in no way a requirement on the
order of the tags. When this tags list is built the preprocessor uses the
index() method on the TagRepository to create the opening_tags and closing_tags
dictionaries, which will look as follows:</p>

<pre class="example indent">
{ 1: [ &lt;Tag s id=s1 1:12 {}>,
       &lt;Tag lex id=l1 1:5 {'lemma': 'Fido', 'pos': 'NNP'}>,
       &lt;Tag ng id=c1 1:5 {}> ],
  6: [ &lt;Tag lex id=l2 6:11 {'lemma': 'bark', 'pos': 'VBZ'}>,
       &lt;Tag vg id=c2 6:11 {}> ],
 11: [ &lt;Tag lex id=l3 11:12 {'lemma': '.', 'pos': '.'}> ] }
</pre>

<pre class="example indent">
{  5: { 1: {'lex': True, 'ng': True}}
  11: { 6: {'lex': True, 'vg': True}},
  12: { 1: {'s': True},
       11: {'lex': True}} }
</pre>



<!----------------------------------------------------------------------------->
<a name="gutime"/>
<h2>4. GUTime</h2>
<!----------------------------------------------------------------------------->

<p>The GUTimeWrapper takes the content of all TarsqiDocElements in the
TarsqiDocument and creates the input needed by components/gutime/TimeTag.pl,
which is the wrapper around TempEx.pm in the same directory. The input required
by TimeTag.pl is a file with content as follows:</p>

<pre class="example indent">
&lt;DOC>
&lt;DATE>20160102&lt;/DATE>
&lt;s>
   &lt;lex id="l1" begin="1" end="5" pos="NNP">Fido&lt;/lex>
   &lt;lex id="l2" begin="6" end="11" pos="NNS">barks&lt;/lex>
   &lt;lex id="l3" begin="12" end="14" pos="IN">on&lt;/lex>
   &lt;lex id="l4" begin="15" end="21" pos="NNP">Monday&lt;/lex>
   &lt;lex id="l5" begin="21" end="22" pos=".">.&lt;/lex>
&lt;/s>
&lt;/DOC>
</pre>

<p>The DOC root and the DATE tag are required, the latter being the way that the
DCT is handed to GUTime. Otherwise, only s and lex tags are allowed. GUTime does
not require the lex tags to have the begin and end attributes, but it is okay
for them to be there. Any kind of spacing between the tags is allowed. The
wrapper creates the above file in a temporary data directory data/tmp (which is
emptied at the beginning of each tarsqi.py invocation) and then uses the Python
subprocess module to run the Perl script. The output is put in the same
temporary directory and is exactly like the input except that TIMEX3 tags are
added:</p>

<pre class="example indent">
&lt;DOC>
&lt;DATE>&lt;TIMEX3 VAL="20160102">20160102&lt;/TIMEX3>&lt;/DATE>
&lt;s>
   &lt;lex id="l1" begin="1" end="5" pos="NNP">Fido&lt;/lex>
   &lt;lex id="l2" begin="6" end="11" pos="NNS">barks&lt;/lex>
   &lt;lex id="l3" begin="12" end="14" pos="IN">on&lt;/lex>
   &lt;TIMEX3 tid="t1" TYPE="DATE">&lt;lex id="l4" begin="15" end="21" pos="NNP">Monday&lt;/lex>&lt;/TIMEX3>
   &lt;lex id="l5" begin="21" end="22" pos=".">.&lt;/lex>
&lt;/s>
&lt;/DOC>
</pre>

<p>Similar to what happened with the preprocessor results, the new TIMEX3 tags
are exported to the tarsqi_tags TagRepository on the TarsqiDocElement. One
difference is that the GUTimeWrapper adds tags using the add_tag method on
TagRepository, which also adds the tag to the opening_tags and closing tags
dictionaries, so a separate use of index() is not needed.</p>


<!----------------------------------------------------------------------------->
<a name="evita"/>
<h2>5. Evita</h2>
<!----------------------------------------------------------------------------->

<p>The EvitaWrapper class is handed the TarsqiDocument and loops over all
TarsqiDocElements in it, creating an Evita instance for all of them and then
procesing the element. The Evita instance has slots for the TarsqiDocument, the
TarsqiDocElement and a TarsqiTree instance which contains a document tree for
the TarsqiDocElement that is being processed. The TarsqiTree instance itself
knows what TarsqiDocument it belongs to and what TarsqiDocElement it was created
for since these were handed in from the Evita instance. The TarsqiTree instance
also has a list of daughters as well as some other attributes that are ignored
here because they are not used by Evita.</p>

<table class="pythonclass indent spacy">
<tr>
  <td class="name" colspan="2">components.evita.main.Evita</td>
</tr>
<tr>
  <td class="attribute">tarsqidoc</td>
  <td>an instance of docmodel.document.TarsqiDocument</td>
</tr>
<tr>
  <td class="attribute">docelement</td>
  <td>an instance of docmodel.document.TarsqiDocElement or subclass thereof, the
  element of the TarsqiDocument that is being processed by Evita</td>
</tr>
<tr>
  <td class="attribute">doctree</td>
  <td>
    <table class="pythonclass embedded">
      <tr>
	<td class="name" colspan="2">components.common_modules.tree.TarsqiTree</td>
      </tr>
      <tr>
	<td class="attribute">tarsqidoc</td>
	<td>an instance of docmodel.document.TarsqiDocument</td>
      </tr>
      <tr>
	<td class="attribute">docelement</td>
	<td>the TarsqDocElement that the document tree is created for</td>
      </tr>
      <tr>
	<td class="attribute">dtrs</td>
	<td>a list of daughters, typically instances of Sentence</td>
      </tr>
    </table>
  </td>
</tr>
</table>

<p>Since the TarsqiTree and its elements are the starting point for Evita and
Slinket processing we will dwell on them a bit longer. Here is a pretty print of
the TarsqiTree for "The dog barked yesterday.".</p>

<pre class="example indent">
&lt;Sentence position=0>
  &lt;NounChunk position=0 checkedEvents=False event=None eid=None>
    &lt;Token position=0 pos=DT text=The>
    &lt;Token position=1 pos=NN text=dog>
  &lt;VerbChunk position=1 checkedEvents=False event=None eid=None>
    &lt;Token position=0 pos=VBD text=barked>
  &lt;NounChunk position=2 checkedEvents=False event=None eid=None>
    &lt;TIMEX3 tid=t1 type=DATE value=20160103>
      &lt;Token position=0 pos=NN text=yesterday>
  &lt;Token position=3 pos=. text=.>
</pre>

<p>A TarsqiTree contains sentences, chunks, tokens and event and time
constituents. It is created from a TarsqiDocElement as a first processing step
by using the create_tarsqi_tree() method in components.common_modules.tree. This
method uses the intermediary Node object to create a tree hierarchy and then
replaces all Node objects with instances of Sentence, NounChunk, VerbChunk,
Token, AdjectiveToken, EventTag and TimexTag (all defined in submodules of
components.common_modules). These tree elements are all subclasses of
Constituent and have the following instance variables:</p>

<table class="pythonclass indent spacy">
  <tr>
    <td class="name" colspan="2">components.common_modules.constituent.Constituent</td>
  </tr>
  <tr>
    <td>tree</td>
    <td>contains the TarsqiTree instance at the top of the tree, through this
    tree a constituent also has access to the TarsqiDocument and the
    TarsqiDocElement</td>
  </tr>
  <tr>
    <td>parent</td>
    <td>a reference to the parent, which is an instance of TarsqiTree or one of
      the subclasses of Constituent</td>
  </tr>
  <tr>
    <td>position</td>
    <td>an integer reflecting the constituent's position in the dtrs list of the
    parent</td>
  </tr>
  <tr>
    <td>dtrs</td>
    <td>a list of daughters, this is the empty list for leaf nodes</td>
  </tr>
  <tr>
    <td>begin</td>
    <td>the beginning offset in the SourceDoc</td>
  </tr>
  <tr>
    <td>end</td>
    <td>the ending offset in the SourceDoc</td>
  </tr>
</table>

<p>The TarsqiTree is used by Tarsqi components as a common data structure to
process over. For example, Evita and Slinket both run patterns over elements of
this tree and major parts of the Evita and Slinket code are expressed as methods
on constituents. Components may update elements of the tree, but it is important
to note that those changes are incidental and will not be handed over to a next
component in the pipeline. Instead, results from processing have to be exported
to the TagRepository on the TarsqiDocElement, as in the following figure.</p>

<img src="images/component.png" width="550"/>

<p>Components import he TarsqiDocument and the TagRepository in the
TarsqiDocElement and create a TarsqiTree from them, they then use this tree as
input to processing and export the resulting tags back into the
TagRepository. The next component in the pipeline will start afresh with a new
TarsqiTree which will be created from the updated TagRepository.</p>

<p>Getting back to Evita...</p>

<p>The EvitaWrapper takes the TarsqiDocument on initialization and its process()
method loops over all TarsqiDocElements, creating an Evita instance for each of
them and then running the process_element() method. The process_elements()
method loops through all Sentence instances in the TarsqiTree in Evita's doctree
variable, and then loops through all daughters of the Sentence. For each
daughter it attempts to create an event with the createEvent() method, which is
implemented on the NounChunk and VerbChunk classes.</p>

<p>As an illustration, below is a fragment from the code in the Evita class in
components.evita.main. Note that the code is slightly simplified and edited and
that it is not well-formed Python code anymore, this is true for all code
fragments shown in this document.</p>

<pre class="example indent">
<i>components.evita.main.Evita</i>

process_element():
   self.doctree = create_tarsqi_tree(self.docelement)
   for sentence in self.doctree:
      for node in sentence:
         if not node.checkedEvents:
            node.createEvent()
</pre>

<p>Nominal events are created with createEvent() on NounChunk and verbal and
adjectival events are created with createEvent() on VerbChunk. Adjectival events
are initially dealt with by VerbChunk because they are created only if preceded
by certain verb groups, which is recognized by VerbChunk. The following three
sections give details on how the three kinds of events are created.</p>


<h3>Nominal events</h3>
<!-------------------->

<p>Most of the code that deals with nominal events is expressed on the NounChunk
class, which is a sublass of Chunk and Constituent. Recall that Constituents
defines instance variables tree, parent, position, dtrs, being and end. These
are all filled in during TarsqiTree construction with create_tarsqi_tree(), as
mentioned earlier in this section. Several other instance variables are defined
on chunks. The table below has just the variables added by NounChunk that are
relevant for Evita.</p>

<table class="pythonclass indent spacy">
  <tr>
    <td class="name" colspan="2">components.common_modules.chunks.NounChunk</td>
  </tr>
  <tr>
    <td>phraseType</td>
    <td>the chunktype, basically the chunk tag generated by the chunker, always
    'ng' for noun chunks</td>
  </tr>
  <tr>
    <td>head</td>
    <td>an integer reflecting the position of the head token in the chunk's dtrs
    variable, always set to -1 for noun chunks, picking out the last element</td>
  </tr>
  <tr>
    <td>gramchunk</td>
    <td>
      <table class="pythonclass embedded">
	<tr>
	  <td class="name" colspan="2">components.evita.gramChunk.GramNChunk</td>
	</tr>
	<tr>
	  <td>node</td>
	  <td>the NounChunk that the features are for</td>
	</tr>
	<tr>
	  <td>tense</td>
	  <td>the tense of the nominal, by default set to 'NONE'</td>
	</tr>
	<tr>
	  <td>aspect</td>
	  <td>the aspect of the nominal, by default set to 'NONE'</td>
	</tr>
	<tr>
	  <td>modality</td>
	  <td>the modality of the nominal, by default set to 'NONE'</td>
	</tr>
	<tr>
	  <td>polarity</td>
	  <td>the polarity of the nominal, by default set to 'POS'</td>
	</tr>
      </table>
    </td>
  </tr>
  <tr>
    <td>gramchunks</td>
    <td>the empty list for noun chunks, but can be non-empty for verb chunks</td>
  </tr>
  <tr>
    <td>checkedEvents</td>
    <td>a flag indicating whether the chunk has already been checked for events,
    initially set to False; if set to True, createEvent() will never be run for
    the chunk</td>
  </tr>
</table>

<p>The GramNChunk instance in the gramchunk variable contains the grammatical
features for the noun chunk. This instance has a pointer back to the NounChunk
it is in and four grammatical features. Three of them are typically 'NONE' for
nouns, and it may sound a wee bit odd for nouns to have tense, aspect or
modality, nominal, but in some cases nominal will inherit these from a governing
verb, for example, with phrases like "This would be a tragedy". When the verb
chunk "be" tries to create an event it will recognize that the actual event is
the nominal following it, it will then try to create an event on that nominal
and hand it the verb chunk features. This is explanined further below.</p>

<p>The top-level of the createEvent() code for nominals looks as follows:</p>

<pre class="example indent">
<i>components.common_modules.chunks.NounChunk</i>

createEvent(self, gramvchunk=None):
   self.gramchunk = GramNChunk(self, gramvchunk)
   if self.isEventCandidate():
      self._processEventInChunk()
</pre>

<p>GramNchunk creation for the NounChunk is a simple affair, setting the
defaults and then leting them be overwritten if features were handed in from the
governing verb. The main work occurs in the isEventCandidate() method on the
chunk, which first checks whether the noun chunk is syntactically able to be an
event, which is the case if the chunk has a head, the head is a common noun and
the chunk is not a time expression. Next, it checks whether the noun chunk can
semantically be an event. It does this by looking up the head token in Wordnet
and by running a simple Bayesian classifier. This is where the heavy lifting
occurs. The general logic is as follows:</p>

<ol>

<li>If all senses of the head token of the chunk are events in WordNet, return
True.</li>

<li>Else, if the classifier has enough data for the head token in its
statistical model, let the classifier decide whether the token is an event.</li>

<li>Else, do another WordNet lookup and check either whether the primary sense
is an event or whether some senses are events.</li>

</ol>

<p>The exact application of this logic is driven by settings in the
components.evita.settings module, which has a couple of variable that determine
whether the Bayesian classifier is used and how WordNet lookup determines
event-hood.

<pre class="example indent">
<i>components.evita.settings</i>

EVITA_NOM_DISAMB = True
EVITA_NOM_CONTEXT = True
EVITA_NOM_WNPRIMSENSE_ONLY = True
</pre>

<p>The first variable determines whether the classifier will actually apply and
the second whether the classifier will take into account contextual features (as
opposed to just the word). The third variable determines the choice to be made
in step 3, the default is to decide that a token is an event if its primary
sense is.</p>

<p>The resources used by the logic above are in library/evita/dictionaries.</p>

<p>If the isEventCandidate() method returns true then the _processEventInChunk()
method will perform a few last checks and then add an event to the
TarsqiDocElement by asking the TarsqiTree to do the honours:</p>

<pre class="example indent">
<i>components.common_modules.chunks.Chunk</i>

def _processEventInChunk(self, gramChunk=None):
     gchunk = self.gramchunk if gramChunk is None else gramChunk
     if (gchunk.head
         and gchunk.head.getText() not in forms.be
         and gchunk.head.getText() not in forms.spuriousVerb
         and gchunk.evClass):
         self.tree.addEvent(Event(gchunk))
</pre>

<p>This method is defined on Chunk and is used by VerbChunks as well. The core
is to invoke addEvent() on the TarsqiTree instance in the tree variable, handing
it an Event instance which was created from the GramNChunk (or gramVChunk for
verb chunks).</p>

<pre class="example indent">
<i>components.common_modules.tree.TarsqiTree</i>

 def addEvent(self, event):
     event_attrs = dict(event.attrs)
     eid = self.tarsqidoc.next_event_id()
     eiid = "ei%s" % eid[1:]
     event_attrs['eid'] = eid
     event_attrs['eiid'] = eiid
     token = event.tokens[-1]
     self.docelement.add_event(token.begin, token.end, event_attrs)
</pre>

<p>The TarsqiTree instance is handed most event features (tense and aspect and
so on), but it is responsible for generating unique identifiers for the event
(which it gets from the TarsqiDocument). Finally, it asks the TarsqiDocElement
to add the tag to the tarsqi_tags TagRepository, thereby exporting the component
results as shown in the picture earlier in this section.</p>


<h3>Verbal events</h3>
<!------------------->


<h3>Adjectival events</h3>
<!------------------->


<!----------------------------------------------------------------------------->
<a name="slinket"/>
<h2>6. Slinket</h2>
<!----------------------------------------------------------------------------->

<table class="pythonclass indent spacy">
<tr>
  <td class="name" colspan="2">components.common_modules.tree.TarsqiTree</td>
</tr>
<tr>
  <td class="attribute">tarsqidoc</td>
  <td>an instance of docmodel.document.TarsqiDocument</td>
</tr>
<tr>
  <td class="attribute">docelement</td>
  <td>the TarsqiDocElement that the tree is created for</td>
</tr>
 <tr>
  <td class="attribute">dtrs</td>
  <td>a list of daughters, typically instances of Sentence</td>
</tr>
<tr>
  <td class="attribute">events</td>
  <td>a dictionary (TODO: indexed on...) containing events for the
  TarsqiDocElement that were found by Evita, it is created by Slinket by
  collecting events and their attributes from the document tree in dtrs</td>
</tr>
<tr>
  <td class="attribute">alink_list</td>
  <td>a list of AlinkTags</td>
</tr>
<tr>
  <td class="attribute">slink_list</td>
  <td>a list of SlinkTags</td>
</tr>
<tr>
  <td class="attribute">tlink_list</td>
  <td>a list of TlinkTags</td>
</tr>
</table>


</body>
</html>
